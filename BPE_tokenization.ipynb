{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the sample implementation of BPE in tiktoken (https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py)\n",
    "# It is modified to work with our code.\n",
    "\n",
    "\n",
    "\"\"\"This is an educational implementation of the byte pair encoding algorithm.\"\"\"\n",
    "import collections\n",
    "import regex\n",
    "\n",
    "gpt2_regex = (r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?[\\p{L}]+| ?[\\p{N}]+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\" )\n",
    "gpt4_regex = (r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "class SimpleBytePairEncoding:\n",
    "    def __init__(self, *, mergeable_ranks: dict[bytes, int]) -> None:\n",
    "        \"\"\"Creates an Encoding object.\"\"\"\n",
    "        # A regex pattern string that is used to split the input text\n",
    "        self.pat_str = gpt4_regex\n",
    "        # A dictionary mapping token bytes to their ranks. The ranks correspond to merge priority\n",
    "        self.mergeable_ranks = mergeable_ranks\n",
    "\n",
    "        self._decoder = {token: token_bytes for token_bytes, token in mergeable_ranks.items()}\n",
    "        self._pat = regex.compile(gpt4_regex)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"Encodes a string into tokens.\n",
    "\n",
    "        >>> enc.encode(\"hello world\")\n",
    "        [388, 372]\n",
    "        \"\"\"\n",
    "        # Use the regex to split the text into (approximately) words\n",
    "        words = self._pat.findall(text)\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            # Turn each word into tokens, using the byte pair encoding algorithm\n",
    "            word_bytes = word.encode(\"utf-8\")\n",
    "            word_tokens = bpe_encode(self.mergeable_ranks, word_bytes)\n",
    "            tokens.extend(word_tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def decode_bytes(self, tokens: list[int]) -> bytes:\n",
    "        \"\"\"Decodes a list of tokens into bytes.\n",
    "\n",
    "        >>> enc.decode_bytes([388, 372])\n",
    "        b'hello world'\n",
    "        \"\"\"\n",
    "        return b\"\".join(self._decoder[token] for token in tokens)\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Decodes a list of tokens into a string.\n",
    "\n",
    "        Decoded bytes are not guaranteed to be valid UTF-8. In that case, we replace\n",
    "        the invalid bytes with the replacement character \"ï¿½\".\n",
    "\n",
    "        >>> enc.decode([388, 372])\n",
    "        'hello world'\n",
    "        \"\"\"\n",
    "        return self.decode_bytes(tokens).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    @staticmethod\n",
    "    def train(training_data: str, vocab_size: int):\n",
    "        \"\"\"Train a BPE tokeniser on some data!\"\"\"\n",
    "        mergeable_ranks = bpe_train(data=training_data, vocab_size=vocab_size)\n",
    "        return SimpleBytePairEncoding(mergeable_ranks=mergeable_ranks)\n",
    "\n",
    "\n",
    "def bpe_encode(mergeable_ranks: dict[bytes, int], input: bytes) -> list[int]:\n",
    "    parts = [bytes([b]) for b in input]\n",
    "    while True:\n",
    "\n",
    "        # Iterate over all pairs and find the pair we want to merge the most\n",
    "        min_idx = None\n",
    "        min_rank = None\n",
    "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "\n",
    "        # If there were no pairs we could merge, we're done!\n",
    "        if min_rank is None:\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "\n",
    "        # Otherwise, merge that pair and leave the rest unchanged. Then repeat.\n",
    "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2 :]\n",
    "\n",
    "    tokens = [mergeable_ranks[part] for part in parts]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def bpe_train(data: str, vocab_size: int) -> dict[bytes, int]:\n",
    "    # First, add tokens for each individual byte value\n",
    "    if vocab_size < 2**8:\n",
    "        raise ValueError(\"vocab_size must be at least 256, so we can encode all bytes\")\n",
    "    ranks = {}\n",
    "    for i in range(2**8):\n",
    "        ranks[bytes([i])] = i\n",
    "\n",
    "    # Splinter up our data into lists of bytes\n",
    "    # data = \"Hello world\"\n",
    "    # words = [\n",
    "    #     [b'H', b'e', b'l', b'l', b'o'],\n",
    "    #     [b' ', b'w', b'o', b'r', b'l', b'd']\n",
    "    # ]\n",
    "    words: list[list[bytes]] = [\n",
    "        [bytes([b]) for b in word.encode(\"utf-8\")] for word in regex.findall(gpt4_regex, data)\n",
    "    ]\n",
    "\n",
    "    # Now, use our data to figure out which merges we should make\n",
    "    while len(ranks) < vocab_size:\n",
    "        # Find the most common pair. This will become our next token\n",
    "        stats = collections.Counter()\n",
    "        for piece in words:\n",
    "            for pair in zip(piece[:-1], piece[1:]):\n",
    "                stats[pair] += 1\n",
    "\n",
    "        most_common_pair = max(stats, key=lambda x: stats[x])\n",
    "        token_bytes = most_common_pair[0] + most_common_pair[1]\n",
    "        token = len(ranks)\n",
    "        # Add the new token!\n",
    "        ranks[token_bytes] = token\n",
    "\n",
    "        # Now merge that most common pair in all the words. That is, update our training data\n",
    "        # to reflect our decision to make that pair into a new token.\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word) - 1:\n",
    "                if (word[i], word[i + 1]) == most_common_pair:\n",
    "                    # We found our pair! Merge it\n",
    "                    new_word.append(token_bytes)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            if i == len(word) - 1:\n",
    "                new_word.append(word[i])\n",
    "            new_words.append(new_word)\n",
    "        words = new_words\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_sequences_pytorch(sequences, maxlen, padding_value=0):\n",
    "    tensor_list = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "    padded_seqs = pad_sequence(tensor_list, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    if padded_seqs.size(1) < maxlen: padded_seqs = F.pad(padded_seqs, (maxlen - padded_seqs.size(1), 0), value=padding_value)\n",
    "    else: padded_seqs = padded_seqs[:, -maxlen:]\n",
    "        \n",
    "    return padded_seqs\n",
    "\n",
    "\n",
    "def createInputOutput(contents , tokenizer, step_size=10 ):\n",
    "    input_seq = []\n",
    "    output_seq = []\n",
    "\n",
    "    encoded_content = [tokenizer.encode(content) for content in contents]\n",
    "    max_encoded_content_length = max([len(i) for i in encoded_content])\n",
    "\n",
    "    for poems in encoded_content:\n",
    "        for position in range(0,len(poems)-1,step_size):\n",
    "            new_input = poems[:position]\n",
    "            new_output = poems[position]\n",
    "            \n",
    "            padded_input = pad_sequences_pytorch([new_input], maxlen=max_encoded_content_length, padding_value=0)\n",
    "\n",
    "            input_seq.append(padded_input[0])\n",
    "            output_seq.append(new_output)\n",
    "\n",
    "    return torch.stack(input_seq), F.one_hot(torch.tensor(output_seq , dtype=torch.long), num_classes=len(tokenizer.mergeable_ranks)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.bilstm1 = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.bilstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)  \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "    \n",
    "class ModifiedBiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(ModifiedBiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.bilstm1 = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.bilstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.9)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)      \n",
    "        self.relu = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x[:, -1, :]   \n",
    "        x = self.fc1(x)   \n",
    "        x = self.relu(x)  \n",
    "        x = self.fc2(x)   \n",
    "        return x\n",
    "    \n",
    "class OnlineLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(OnlineLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.bilstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = len(tokenizer.mergeable_ranks)\n",
    "embed_dim = 10  \n",
    "hidden_dim = 15 \n",
    "output_dim = vocab_size  \n",
    "\n",
    "# model = BiLSTMModel(vocab_size, embed_dim, hidden_dim , output_dim).to(device)\n",
    "model = OnlineLSTMModel(vocab_size, embed_dim, hidden_dim , output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # type: ignore\n",
    "\n",
    "train_dataset = TensorDataset(input_seq.to(device), output_seq.to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(inp, temperature , loop):\n",
    "    some_input_data = tokenizer.encode(inp)\n",
    "    some_input_data = pad_sequences_pytorch([some_input_data], maxlen=input_seq.shape[1], padding_value=0)\n",
    "\n",
    "    input_data = torch.tensor(some_input_data, dtype=torch.long).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad(): outputs = model(input_data)\n",
    "\n",
    "    logits = outputs / temperature\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    if (loop == 0):\n",
    "        top_values, top_indices = torch.topk(outputs, 10, dim=-1)\n",
    "\n",
    "        sorted_top_values, sorted_indices = top_values.sort(descending=True)\n",
    "        sorted_top_indices = top_indices.gather(dim=-1, index=sorted_indices)\n",
    "\n",
    "        print(\"Top 10 highest values and their corresponding indices in the tensor:\")\n",
    "        for i in range(10): print(f\"Value: {sorted_top_values[0][i].item()}, Index: {sorted_top_indices[0][i].item()} , Token: {tokenizer.decode([sorted_top_indices[0][i].item()])}\") #type: ignore\n",
    "\n",
    "\n",
    "    next_word_index = torch.multinomial(probs[0], num_samples=1).item()\n",
    "    response = tokenizer.decode([next_word_index])  # type: ignore\n",
    "\n",
    "    # print(inp + response)\n",
    "    return response\n",
    "\n",
    "inp = \"\"\n",
    "for i in range(50):\n",
    "    outs = generate(inp, 0.25 , i) \n",
    "    inp = inp + \" \" + outs\n",
    "\n",
    "print('\\n',inp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
