{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader , random_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset = json.load(open('Robert_Frost_Poem_Collections.json'))\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "for item in dataset:\n",
    "  titles.append(item.get('title'))\n",
    "  contents.append(item.get('content'))\n",
    "\n",
    "training_text = ''.join(contents)\n",
    "\n",
    "with open('training_text.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab_size= 3000, model_file='spm_vocab.model'):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_file = model_file\n",
    "        self.special_tokens = {\n",
    "            '<PADDING>': 0,\n",
    "            '<STARTLINE>': 1,\n",
    "            '<ENDLINE>': 2,\n",
    "            '<UNKNOWN>': 3\n",
    "        }\n",
    "        self.tokenizer = None\n",
    "        self.train()\n",
    "        self.tokenizer = spm.SentencePieceProcessor(model_file=self.model_file)     # type: ignore\n",
    "        self.max_pad_length = self._find_max_token_length('training_text.txt')\n",
    "        self._build_vocab()\n",
    "\n",
    "    def train(self):\n",
    "        spm.SentencePieceTrainer.train(                                             # type: ignore\n",
    "            input='training_text.txt',\n",
    "            model_prefix='spm_vocab',\n",
    "            vocab_size=self.vocab_size,\n",
    "            model_type='bpe',\n",
    "            pad_id=self.special_tokens['<PADDING>'],\n",
    "            unk_id=self.special_tokens['<UNKNOWN>'],\n",
    "            bos_id=self.special_tokens['<STARTLINE>'],\n",
    "            eos_id=self.special_tokens['<ENDLINE>'],\n",
    "            user_defined_symbols=list(self.special_tokens.keys())\n",
    "        )\n",
    "\n",
    "    def _find_max_token_length(self, file_path):\n",
    "        max_length = 0\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = self.tokenizer.encode_as_pieces(line)                     # type: ignore\n",
    "                max_length = max(max_length, len(tokens))\n",
    "\n",
    "        return max(max_length,40)\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        self.vocab = {token: idx for token, idx in self.special_tokens.items()}\n",
    "\n",
    "        for id in range(self.tokenizer.get_piece_size()):                           # type: ignore\n",
    "            piece = self.tokenizer.id_to_piece(id)                                  # type: ignore\n",
    "            if piece not in self.vocab:\n",
    "                self.vocab[piece] = len(self.vocab)\n",
    "\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text, method=\"int\", pad_output=False):\n",
    "        if method == \"int\": encoded = [self.special_tokens['<STARTLINE>']] + self.tokenizer.encode_as_ids(text) + [self.special_tokens['<ENDLINE>']]       # type: ignore\n",
    "        else: encoded = ['<STARTLINE>'] + self.tokenizer.encode_as_pieces(text) + ['<ENDLINE>']             # type: ignore\n",
    "\n",
    "\n",
    "        if pad_output:\n",
    "            encoded = encoded[:self.max_pad_length]\n",
    "            padding_needed = self.max_pad_length - len(encoded)\n",
    "            if padding_needed > 0:\n",
    "                if (method == \"int\") : encoded += [self.special_tokens['<PADDING>']] * padding_needed\n",
    "                else : encoded += ['<PADDING>'] * padding_needed\n",
    "\n",
    "        return encoded\n",
    "\n",
    "\n",
    "    def decode(self, ids, method=\"int\"):\n",
    "        if method == \"int\":\n",
    "            ids = [id for id in ids if id not in [self.special_tokens['<STARTLINE>'], self.special_tokens['<ENDLINE>']]]\n",
    "            return self.tokenizer.decode_ids(ids)                                                                           # type: ignore\n",
    "        else:\n",
    "            pieces = [piece for piece in ids if piece not in ['<STARTLINE>', '<ENDLINE>', '<PADDING>']]\n",
    "            return self.tokenizer.decode_pieces(pieces)                                                                     # type: ignore\n",
    "\n",
    "    def create_input_output(self, dataset):\n",
    "        inputs = []\n",
    "\n",
    "        for lines in dataset:\n",
    "            encoded_sentences = self.encode(lines, pad_output=True)\n",
    "            inputs.append(torch.tensor(encoded_sentences, dtype=torch.long))\n",
    "\n",
    "        inputs = torch.stack(inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data shape is  : torch.Size([2507, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: training_text.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_vocab\n",
      "  model_type: BPE\n",
      "  vocab_size: 3000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <PADDING>\n",
      "  user_defined_symbols: <STARTLINE>\n",
      "  user_defined_symbols: <ENDLINE>\n",
      "  user_defined_symbols: <UNKNOWN>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: training_text.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 3085 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PADDING>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <STARTLINE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <ENDLINE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <UNKNOWN>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=110579\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9593% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=57\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999593\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 2802 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 2802\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 5146\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2943 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=690 size=20 all=1046 active=988 piece=▁l\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=416 size=40 all=1556 active=1498 piece=▁\"\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=278 size=60 all=1937 active=1879 piece=ke\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=196 size=80 all=2287 active=2229 piece=▁that\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=100 all=2593 active=2535 piece=▁He\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=152 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=120 all=2778 active=1182 piece=▁You\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=98 size=140 all=3012 active=1416 piece=one\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=88 size=160 all=3212 active=1616 piece=?\"\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=180 all=3370 active=1774 piece=ant\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=200 all=3514 active=1918 piece=ie\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=62 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=220 all=3741 active=1204 piece=▁'\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=240 all=3878 active=1341 piece=▁fl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=260 all=4017 active=1480 piece=and\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=280 all=4101 active=1564 piece=▁com\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=300 all=4204 active=1667 piece=▁more\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=37 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=320 all=4305 active=1102 piece=▁se\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=340 all=4386 active=1183 piece=est\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=360 all=4520 active=1317 piece=▁find\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=380 all=4587 active=1384 piece=▁ch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=400 all=4696 active=1493 piece=▁de\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=420 all=4790 active=1087 piece=▁Not\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=440 all=4933 active=1230 piece=led\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=460 all=4980 active=1277 piece=ag\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=480 all=5059 active=1356 piece=▁looked\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=500 all=5146 active=1443 piece=▁J\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=19 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=520 all=5225 active=1072 piece=▁love\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=540 all=5294 active=1141 piece=▁Like\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=560 all=5348 active=1195 piece=▁past\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=580 all=5414 active=1261 piece=▁own\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=600 all=5419 active=1266 piece=ged\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=620 all=5484 active=1057 piece=▁mind\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=640 all=5503 active=1076 piece=▁ac\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=660 all=5535 active=1108 piece=▁field\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=680 all=5587 active=1160 piece=raid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=700 all=5606 active=1179 piece=▁myself\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=720 all=5672 active=1067 piece=▁Don\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=740 all=5689 active=1084 piece=▁afraid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=760 all=5762 active=1157 piece=ince\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=780 all=5780 active=1175 piece=▁wife\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=800 all=5781 active=1176 piece=mon\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=820 all=5852 active=1067 piece=▁sit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=840 all=5850 active=1065 piece=▁learn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=860 all=5870 active=1085 piece=ity\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=880 all=5911 active=1126 piece=▁lau\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=900 all=5935 active=1150 piece=▁doing\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=920 all=5921 active=987 piece=ab\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=940 all=5984 active=1050 piece=▁isn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=960 all=5998 active=1064 piece=▁stor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=980 all=5990 active=1056 piece=ap\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=1000 all=6061 active=1127 piece=▁tw\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=1020 all=6119 active=1053 piece=▁mow\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=1040 all=6133 active=1067 piece=▁kitc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=1060 all=6133 active=1067 piece=▁stone\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=1080 all=6118 active=1052 piece=▁hundred\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=1100 all=6165 active=1099 piece=ene\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=1120 all=6214 active=1047 piece=lled\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=1140 all=6235 active=1068 piece=round\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=1160 all=6234 active=1067 piece=▁vill\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=1180 all=6238 active=1071 piece=▁tired\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=1200 all=6220 active=1053 piece=▁trouble\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=1220 all=6255 active=1036 piece=ash\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=1240 all=6292 active=1073 piece=cept\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=1260 all=6319 active=1100 piece=▁bar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=1280 all=6352 active=1133 piece=ished\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=1300 all=6353 active=1134 piece=▁neck\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=1320 all=6353 active=1001 piece=▁diffe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=1340 all=6341 active=989 piece=▁caught\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=1360 all=6327 active=975 piece=▁village\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1380 all=6355 active=1003 piece=Let\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1400 all=6387 active=1035 piece=nce\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1420 all=6421 active=1032 piece=▁ye\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1440 all=6453 active=1064 piece=ixed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1460 all=6464 active=1075 piece=▁hot\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1480 all=6492 active=1103 piece=iness\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1500 all=6492 active=1103 piece=▁cows\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1520 all=6485 active=994 piece=▁sake\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1540 all=6481 active=990 piece=▁clerk\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1560 all=6467 active=976 piece=▁throw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1580 all=6458 active=967 piece=▁family\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1600 all=6442 active=951 piece=▁thrust\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=1620 all=6425 active=983 piece=▁Something\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1640 all=6422 active=980 piece=aid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1660 all=6457 active=1015 piece=not\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1680 all=6495 active=1053 piece=▁Ch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1700 all=6514 active=1072 piece=ging\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1720 all=6543 active=1027 piece=yard\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1740 all=6552 active=1036 piece=▁fin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1760 all=6570 active=1054 piece=ation\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1780 all=6586 active=1070 piece=▁Five\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1800 all=6573 active=1057 piece=▁bran\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1820 all=6568 active=995 piece=▁free\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1840 all=6552 active=979 piece=▁path\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1860 all=6547 active=974 piece=ground\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1880 all=6539 active=966 piece=▁bread\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1900 all=6527 active=954 piece=▁knoll\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1920 all=6513 active=987 piece=▁stall\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1940 all=6500 active=974 piece=▁Behind\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1960 all=6485 active=959 piece=▁living\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=1980 all=6471 active=945 piece=▁whiter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=2000 all=6456 active=930 piece=▁fingers\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=2020 all=6436 active=981 piece=▁spoiled\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=2040 all=6421 active=966 piece=▁watching\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2060 all=6417 active=962 piece=os\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2080 all=6440 active=985 piece=bwe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2100 all=6466 active=1011 piece=isc\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2120 all=6487 active=1020 piece=urd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2140 all=6503 active=1036 piece=Five\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2160 all=6505 active=1038 piece=ccup\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2180 all=6521 active=1054 piece=ited\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2200 all=6538 active=1071 piece=roat\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2220 all=6544 active=1006 piece=▁Off\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2240 all=6540 active=1002 piece=▁die\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2260 all=6539 active=1001 piece=▁len\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2280 all=6541 active=1003 piece=▁tip\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2300 all=6543 active=1005 piece=eered\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2320 all=6555 active=1011 piece=nders\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2340 all=6571 active=1027 piece=spire\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2360 all=6570 active=1026 piece=▁Long\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2380 all=6558 active=1014 piece=▁char\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2400 all=6552 active=1008 piece=▁fanc\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2420 all=6540 active=987 piece=▁loaf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2440 all=6530 active=977 piece=▁rack\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2460 all=6520 active=967 piece=▁stre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2480 all=6519 active=966 piece=eeping\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2500 all=6525 active=972 piece=▁Avery\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2520 all=6510 active=985 piece=▁among\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2540 all=6497 active=972 piece=▁close\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2560 all=6491 active=966 piece=▁front\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2580 all=6474 active=949 piece=▁memor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2600 all=6461 active=936 piece=▁saved\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2620 all=6444 active=984 piece=▁story\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2640 all=6432 active=972 piece=picking\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2660 all=6421 active=961 piece=▁Toward\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2680 all=6403 active=943 piece=▁burned\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2700 all=6388 active=928 piece=▁gently\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2720 all=6371 active=984 piece=▁plates\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2740 all=6354 active=967 piece=▁softer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2760 all=6338 active=951 piece=▁warmth\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2780 all=6320 active=933 piece=▁chamber\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2800 all=6303 active=916 piece=▁mistake\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2820 all=6283 active=981 piece=▁suspect\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2840 all=6264 active=962 piece=▁friendly\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2860 all=6244 active=942 piece=▁Fairbanks\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=2880 all=6224 active=922 piece=▁understanding\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=2900 all=6222 active=920 piece=Rank\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=2920 all=6217 active=996 piece=crib\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: spm_vocab.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_vocab.vocab\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab_size=3000)\n",
    "\n",
    "Text_Dataset = [lines for content in contents for lines in content.split('\\r\\n')]\n",
    "\n",
    "inp = tokenizer.create_input_output(Text_Dataset)\n",
    "print(f\"Input Data shape is  : {inp.shape}\")\n",
    "\n",
    "train_size = int(0.8 * len(inp))\n",
    "test_size = len(inp) - train_size\n",
    "train_data, test_data = random_split(inp, [train_size, test_size])          # type: ignore\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layers, output_size,emb_size = 512):\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size,emb_size)\n",
    "        self.mlb = nn.Sequential(nn.Linear(emb_size,emb_size), nn.LayerNorm(emb_size) , nn.ELU() , nn.Linear(emb_size,emb_size))\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers=layers, batch_first=True, dropout=0.2)\n",
    "        self.out = nn.Sequential(nn.Linear(hidden_size,hidden_size//2), nn.LayerNorm(hidden_size//2), nn.ELU(), nn.Dropout(0.5), nn.Linear(hidden_size//2 , output_size))\n",
    "\n",
    "    def forward(self, input_seq, hidden, mem):\n",
    "        input_emb = self.embedding(input_seq)\n",
    "        input_emb = self.mlb(input_emb)\n",
    "        output , (hidden_out , memory_out) = self.lstm(input_emb,(hidden,mem))\n",
    "\n",
    "        return self.out(output) , hidden_out , memory_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = tokenizer.vocab_size\n",
    "output_size = tokenizer.vocab_size\n",
    "hidden_size = 256\n",
    "layers = 2\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Predictor(input_size, hidden_size, layers, output_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)                #type: ignore\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lost_logs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for text in train_dataloader:\n",
    "        input = text[:,0:-1].to(device)\n",
    "        output = text[:,1:].to(device)\n",
    "\n",
    "        hidden = torch.zeros(layers, input.shape[0], hidden_size).to(device)\n",
    "        cell = torch.zeros(layers, input.shape[0], hidden_size).to(device)\n",
    "\n",
    "        pred,hidden,cell = model(input, hidden, cell)\n",
    "        pred = pred.transpose(1,2)\n",
    "        pred = pred.float()\n",
    "    \n",
    "        loss = loss_fn(pred, output)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        lost_logs.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Average Loss: 1.7506\n",
      "Accuracy: 72.65%\n",
      "Perplexity: 5.76\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval() \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for text in dataloader:\n",
    "            input_seq = text[:, 0:-1].to(device)  \n",
    "            target_seq = text[:, 1:].to(device)  \n",
    "\n",
    "            hidden = torch.zeros(layers, input_seq.shape[0], hidden_size).to(device)\n",
    "            cell = torch.zeros(layers, input_seq.shape[0], hidden_size).to(device)\n",
    "\n",
    "            predictions, hidden, cell = model(input_seq, hidden, cell)\n",
    "\n",
    "            predictions = predictions.transpose(1, 2).float()\n",
    "            loss = loss_fn(predictions, target_seq)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted_indices = torch.max(predictions, dim=1)  \n",
    "            total_correct += (predicted_indices == target_seq).sum().item() \n",
    "            total_tokens += target_seq.numel()  \n",
    "            total_examples += input_seq.shape[0]  \n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "\n",
    "    return avg_loss, accuracy, perplexity\n",
    "\n",
    "avg_loss, accuracy, perplexity = evaluate(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I'm not afraid of what you're going to say.\n",
      "I'm not afraid of it, or you're you.\n",
      "On the windows that one man's all\n",
      "And there's I don't know what he's not safe\n",
      "I'm just there. I've not.\n"
     ]
    }
   ],
   "source": [
    "def sample_from_logits(logits, temperature):\n",
    "    logits = logits / temperature\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "def generate_text(model, start_sequence, max_length, device, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated_sequence = start_sequence\n",
    "    input_seq = torch.tensor(start_sequence).unsqueeze(0).to(device)\n",
    "    hidden = torch.zeros(layers, 1, hidden_size).to(device)\n",
    "    mem = torch.zeros(layers, 1, hidden_size).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad(): output, hidden, mem = model(input_seq, hidden, mem)\n",
    "        \n",
    "        logits = output[:, -1, :]\n",
    "    \n",
    "        predicted_token = sample_from_logits(logits, temperature)\n",
    "        generated_sequence.append(predicted_token)\n",
    "        input_seq = torch.tensor(generated_sequence[-len(start_sequence):]).unsqueeze(0).to(device)\n",
    "\n",
    "    return generated_sequence\n",
    "\n",
    "max_length = 100  \n",
    "temperature = 0.6\n",
    "poem = []\n",
    "\n",
    "for _ in range(5):\n",
    "    start_sequence = [tokenizer.vocab.get('<STARTLINE>')]  \n",
    "    generated_tokens = generate_text(model, start_sequence, max_length, device, temperature)\n",
    "    poem.append(tokenizer.decode(generated_tokens))\n",
    "\n",
    "for lines in poem : print(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
